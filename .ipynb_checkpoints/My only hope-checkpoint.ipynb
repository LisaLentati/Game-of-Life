{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.signal import convolve2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def life_step(X):\n",
    "    nbrs_count = convolve2d(X, np.ones((3, 3)), mode='same', boundary='wrap') - X\n",
    "    return (nbrs_count == 3) | (X & (nbrs_count == 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create smaller samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_pillow(grid):\n",
    "    m,n = grid.shape\n",
    "    A = np.empty((m+2,n+2))\n",
    "    A[1:-1, 1:-1] = grid\n",
    "    A[0,0] = grid[-1,-1]\n",
    "    A[0,-1] = grid[-1,0]\n",
    "    A[-1,0] = grid[0,-1]\n",
    "    A[-1,-1] = grid[0,0]\n",
    "    A[ 1:-1, 0] = grid[:,-1]\n",
    "    A[1 : -1 , -1] = grid[:, 0]\n",
    "\n",
    "    A[0, 1:-1] = grid[-1]\n",
    "    A[-1, 1:-1] = grid[0]\n",
    "    return A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = []\n",
    "samples_p1 = []\n",
    "samples_p2 = []\n",
    "probs = np.random.uniform(0.1, 0.9, 10)\n",
    "for prob in probs:\n",
    "    grid = np.random.binomial(n=1,p=prob, size=(6,6))\n",
    "    for _ in range(2):\n",
    "        grid = life_step(grid)\n",
    "        \n",
    "    if grid.sum() > 0:\n",
    "        samples.append(grid)\n",
    "        p1 = create_pillow(grid)\n",
    "        samples_p1.append(p1)\n",
    "        p2 = create_pillow(p1)\n",
    "        samples_p2.append(p2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "k1 = [[1,1,1], [1,1,1], [1,1,1]]\n",
    "k1 = tf.constant(np.array(k1).astype(float), shape=[3,3,1,1,], dtype=\"float32\")\n",
    "\n",
    "k2 = [[1,1,1,1,1],[1,1,1,1,1],[1,1,1,1,1],[1,1,1,1,1,],[1,1,1,1,1]]\n",
    "k2 = tf.constant(np.array(k2).astype(float), shape=[5,5,1,1,], dtype=\"float32\")\n",
    "\n",
    "inputs = tf.constant(np.array(samples).astype(float), shape= [len(samples),6,6,1], dtype=\"float32\")\n",
    "inputs_p1 = tf.constant(np.array(samples_p1).astype(float), shape= [len(samples),8,8,1], dtype=\"float32\")\n",
    "inputs_p2 = tf.constant(np.array(samples_p2).astype(float), shape= [len(samples),10,10,1], dtype=\"float32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "W0 = tf.Variable(tf.random.uniform(shape=[1,6,6,1], minval=0.4, maxval=0.8))\n",
    "W1 = tf.Variable(tf.random.uniform(shape=[1,6,6,1], minval=0.4, maxval=0.8))\n",
    "W2 = tf.Variable(tf.random.uniform(shape=[1,6,6,1], minval=0.4, maxval=0.8))\n",
    "\n",
    "@tf.function\n",
    "def aa(inputs, inputs_p1, inputs_p2):\n",
    "    conv1 = tf.nn.conv2d(inputs_p1, k1, strides=1, padding='VALID')\n",
    "    conv2 = tf.nn.conv2d(inputs_p2, k2, strides=1, padding='VALID')\n",
    "    \n",
    "    out1 = inputs*W0 + inputs*W1 + inputs*W2\n",
    "    return out1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = aa(inputs, inputs_p1, inputs_p2)\n",
    "\n",
    "#a = tf.reshape(a, shape = [len(samples), 6, 6])\n",
    "#b = tf.reshape(b, shape = [len(samples), 6, 6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "cross_entropy() missing 2 required positional arguments: 'y' and 'y_hat'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-ed2236e75c10>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreduce_mean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0moptimiser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mv1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mGradientDescentOptimizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1e-2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mminimize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcross_entropy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\tensorflow_core\\python\\training\\optimizer.py\u001b[0m in \u001b[0;36mminimize\u001b[1;34m(self, loss, global_step, var_list, gate_gradients, aggregation_method, colocate_gradients_with_ops, name, grad_loss)\u001b[0m\n\u001b[0;32m    401\u001b[0m         \u001b[0maggregation_method\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maggregation_method\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    402\u001b[0m         \u001b[0mcolocate_gradients_with_ops\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcolocate_gradients_with_ops\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 403\u001b[1;33m         grad_loss=grad_loss)\n\u001b[0m\u001b[0;32m    404\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    405\u001b[0m     \u001b[0mvars_with_grad\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mv\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mgrads_and_vars\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mg\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\tensorflow_core\\python\\training\\optimizer.py\u001b[0m in \u001b[0;36mcompute_gradients\u001b[1;34m(self, loss, var_list, gate_gradients, aggregation_method, colocate_gradients_with_ops, grad_loss)\u001b[0m\n\u001b[0;32m    460\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mvar_list\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    461\u001b[0m           \u001b[0mtape\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvar_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 462\u001b[1;33m         \u001b[0mloss_value\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    463\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    464\u001b[0m         \u001b[1;31m# Scale loss if using a \"mean\" loss reduction and multiple replicas.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: cross_entropy() missing 2 required positional arguments: 'y' and 'y_hat'"
     ]
    }
   ],
   "source": [
    "def cross_entropy(y, y_hat):\n",
    "    loss = tf.abs(y-y_hat)\n",
    "    return tf.reduce_mean(loss)\n",
    "\n",
    "loss = cross_entropy(a, )\n",
    "optimiser = tf.compat.v1.train.GradientDescentOptimizer(learning_rate=1e-2).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps = 10000 # total number of iterations\n",
    "with tf.compat.v1.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    loss_list = []\n",
    "    weight_list = []\n",
    "    bias_list = []\n",
    "    for i in range(steps):\n",
    "        _, l, w_0, b_0= sess.run([optimiser, loss, w, b], feed_dict={data_x: x, data_y: y})\n",
    "        loss_list.append(l) # save the loss at each iteration for plot so you can visualise the training process\n",
    "        weight_list.append(w_0)\n",
    "        bias_list.append(b_0)\n",
    "        \n",
    "    w_val, b_val = sess.run([w, b])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
